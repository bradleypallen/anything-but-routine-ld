{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/bradleyallen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/bradleyallen/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/bradleyallen/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/bradleyallen/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "import rdflib, glob, re, nltk, pandas as pd\n",
    "from rdflib import URIRef, BNode, Literal\n",
    "from rdflib.namespace import RDF, RDFS, DCTERMS\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abri = rdflib.Namespace(\"http://bradleypallen.org/anything-but-routine-ld/4.0/instance/\")\n",
    "abrw = rdflib.Namespace(\"http://bradleypallen.org/anything-but-routine-ld/4.0/work/\")\n",
    "bf = rdflib.Namespace(\"http://id.loc.gov/ontologies/bibframe/\")\n",
    "arm = rdflib.Namespace(\"https://w3id.org/arm/core/ontology/0.1/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse all edited .ttl files into a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_abr_graph():\n",
    "    g = rdflib.Graph()\n",
    "    g.bind(\"abri\", \"http://bradleypallen.org/anything-but-routine-ld/4.0/instance/\")\n",
    "    g.bind(\"abrw\", \"http://bradleypallen.org/anything-but-routine-ld/4.0/work/\")\n",
    "    g.bind(\"bf\", \"http://id.loc.gov/ontologies/bibframe/\")\n",
    "    g.bind(\"arm\", \"https://w3id.org/arm/core/ontology/0.1/\")\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABR graph has 9339 triples.\n"
     ]
    }
   ],
   "source": [
    "g = initialize_abr_graph()\n",
    "for infile in glob.glob(\"edited-ttl/*/*.ttl\"):\n",
    "    g.parse(infile, format='n3')\n",
    "n = len(g)\n",
    "print(f\"ABR graph has {n} triples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dataframe with instance promotional materials from notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "materials = []\n",
    "\n",
    "for instance in g.subjects(RDF.type, bf.Instance):\n",
    "    inst_uri = instance.toPython()\n",
    "    inst_label = g.value(instance, RDFS.label)\n",
    "    inst_id = inst_uri[inst_uri.rfind('/')+1:]\n",
    "    for note in g.objects(instance, bf.note):\n",
    "        text = g.value(note, RDF.value).toPython()\n",
    "        if re.search('^1[ab]?. \\[', text):\n",
    "            materials.append([inst_uri, inst_label, inst_id, text])\n",
    "                \n",
    "df = pd.DataFrame(materials, columns=['instanceuri', 'instancelabel', 'instanceid', 'description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract instance attributes and relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['pmuri'] = df['instanceuri'] + df['description'].str.extract('^(\\d[ab]?).', expand=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bf:identifiedBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['pmid'] = df['instanceid'] + df['description'].str.extract('^(\\d[ab]?).', expand=False)\n",
    "df['MaynardAndMiles'] = df['description'].str.extract('\\{M\\&M (\\S*)\\}', expand=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bf:provisionActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "publication_ptrn = ' (\\[?[\\s\\w\\-\\.\\, ]+\\]?: \\[?[\\s\\w\\-\\.\\, ]+\\]?\\, \\[?\\d\\d\\d\\d\\??\\]?)'\n",
    "df['publication'] = df['description'].str.extract(publication_ptrn, expand=False)\n",
    "df['place'] = df['publication'].str.extract('(.*):', expand=False)\n",
    "df['agent'] = df['publication'].str.extract(': (.*),', expand=False)\n",
    "df['date'] = df['publication'].str.extract(', (\\[?\\d\\d\\d\\d\\??\\]?)', expand=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type of promotional material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['type'] = df['description'].str.extract('^\\d[ab]?\\. \\[([ \\w]+)\\]', expand=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dcterms:hasPart arm:Binding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['binding'] = df['description'].str.extract('(Pamphlet)', expand=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dict to iterate over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "instances = df.where((pd.notnull(df)), None).to_dict('records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add bf:Instances for promotional materials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for inst in instances:\n",
    "    name = inst['pmid']\n",
    "    #file = f\"edited-ttl/instance/{name}.ttl\"\n",
    "    file = f\"pmat-test/{name}.ttl\"\n",
    "    h = initialize_abr_graph()\n",
    "    uri = URIRef(inst['pmuri'])\n",
    "    h.add((uri, RDF.type, bf.Instance))\n",
    "    \n",
    "    # rdfs:label\n",
    "    label = inst['type'] + ' for ' + inst['instancelabel']\n",
    "    h.add((uri, RDFS.label, Literal(label)))\n",
    "\n",
    "    # bf.note\n",
    "    sentences = nltk.sent_tokenize(inst['description'])\n",
    "    for sentence in sentences:\n",
    "        note = BNode()\n",
    "        h.add((uri, bf.note, note))\n",
    "        h.add((note, RDF.type, bf.Note))\n",
    "        h.add((note, RDF.value, Literal(sentence)))\n",
    "\n",
    "    # bf.contributor\n",
    "    wsb = BNode()\n",
    "    h.add((uri, bf.contributor, wsb))\n",
    "    h.add((wsb, RDF.type, bf.Agent))\n",
    "    h.add((wsb, RDF.type, bf.Person)) \n",
    "    h.add((wsb, bf.role, Literal(\"author\")))\n",
    "    h.add((wsb, RDFS.label, Literal(\"William S. Burroughs\")))\n",
    "    tokenized = nltk.word_tokenize(inst['description'])\n",
    "    tagged = nltk.pos_tag(tokenized)\n",
    "    namedEnt = nltk.ne_chunk(tagged)\n",
    "    for t in namedEnt.subtrees():\n",
    "        if t.label() == 'PERSON':\n",
    "            person_tokens = [ c[0] for c in t ]\n",
    "            person_name = ' '.join(person_tokens)\n",
    "            person = BNode()\n",
    "            h.add((uri, bf.contributor, person))\n",
    "            h.add((person, RDF.type, bf.Agent))\n",
    "            h.add((person, RDF.type, bf.Person)) \n",
    "            h.add((person, bf.role, Literal(\"contributor\")))\n",
    "            h.add((person, RDFS.label, Literal(person_name)))\n",
    "            \n",
    "    # bf:identifiedBy\n",
    "    identifier = BNode()\n",
    "    h.add((uri, bf.identifiedBy, identifier))\n",
    "    h.add((identifier, RDF.type, bf.Identifier))\n",
    "    h.add((identifier, bf.source, Literal(\"Schottlaender v4.0\")))\n",
    "    h.add((identifier, RDF.value, Literal(inst['pmid'])))\n",
    "    if inst['MaynardAndMiles'] is not None:\n",
    "        m_n_m_id = BNode()\n",
    "        h.add((uri, bf.identifiedBy, m_n_m_id))\n",
    "        h.add((m_n_m_id, RDF.type, bf.Identifier))\n",
    "        h.add((m_n_m_id, bf.source, Literal(\"Maynard & Miles\")))\n",
    "        h.add((m_n_m_id, RDF.value, Literal(inst['MaynardAndMiles'])))\n",
    "        \n",
    "    # bf:provisionActivity\n",
    "    activity = BNode()\n",
    "    h.add((uri, bf.provisionActivity, activity))\n",
    "    h.add((activity, RDF.type, bf.ProvisionActivity))\n",
    "    h.add((activity, RDF.type, bf.Publication))\n",
    "    h.add((activity, bf.agent, Literal(inst['agent'])))\n",
    "    h.add((activity, bf.date, Literal(inst['date'])))\n",
    "    h.add((activity, bf.place, Literal(inst['place'])))\n",
    "\n",
    "    # bf.relatedTo\n",
    "    h.add((uri, bf.relatedTo, URIRef(inst['instanceuri'])))\n",
    "    \n",
    "    # dcterms:hasPart\n",
    "    if inst['binding']:\n",
    "        binding = BNode()\n",
    "        h.add((uri, DCTERMS.hasPart, binding))\n",
    "        h.add((binding, RDF.type, arm.Binding))\n",
    "        note = BNode()\n",
    "        h.add((binding, bf.note, note))\n",
    "        h.add((note, RDF.type, bf.Note))\n",
    "        h.add((note, RDF.type, arm.DescriptiveNote))\n",
    "        h.add((note, RDF.value, Literal(f\"{inst['binding']}.\")))\n",
    "        h.serialize(file, format='turtle')\n",
    "\n",
    "    h.serialize(file, format='turtle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove old note triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for inst in instances:\n",
    "    name = inst['instanceid']\n",
    "    file = f\"edited-ttl/instance/{name}.ttl\"\n",
    "    h = initialize_abr_graph()\n",
    "    h.parse(file, format='n3')\n",
    "    uri = URIRef(inst['instanceuri'])\n",
    "    h.add((uri, bf.relatedTo, URIRef(inst['pmuri'])))\n",
    "    for note in h.objects(uri, bf.note):\n",
    "        text = h.value(note, RDF.value)\n",
    "        if text.toPython() == inst['description']:\n",
    "            h.remove((uri, bf.note, note))\n",
    "            h.remove((note, None, None))\n",
    "    h.serialize(file, format='turtle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:wsbhack]",
   "language": "python",
   "name": "conda-env-wsbhack-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
