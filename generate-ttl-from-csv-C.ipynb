{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages, initialize sentence tokenization resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/bradleyallen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/bradleyallen/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/bradleyallen/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/bradleyallen/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, rdflib, re, nltk, collections\n",
    "from rdflib.namespace import RDF, RDFS\n",
    "from rdflib import URIRef, BNode, Literal\n",
    "from slugify import slugify\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv/C.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract entity names and features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize all whitespace to spaces in notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['description'] = df['description'].str.replace('\\s', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df:title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['title'] = df['description'].str.extract('^(.*\\))\\.', expand=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df:date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['date'] = df['title'].str.extract('([12][90]\\d\\d)', expand=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df:identifiedBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['mnmid'] = df['description'].str.extract('\\{M\\&M (.*)\\}', expand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['abrid'] = df['work']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df:note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['note'] = df['description'].str.extract('^.*\\)\\. (.*)$', expand=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate BIBFRAME Turtle files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abrc = rdflib.Namespace(\"https://w3id.org/anything-but-routine/4.0/classification/\")\n",
    "abri = rdflib.Namespace(\"https://w3id.org/anything-but-routine/4.0/instance/\")\n",
    "abrw = rdflib.Namespace(\"https://w3id.org/anything-but-routine/4.0/work/\")\n",
    "bf = rdflib.Namespace(\"http://id.loc.gov/ontologies/bibframe/\")\n",
    "arm = rdflib.Namespace(\"https://w3id.org/arm/core/ontology/0.1/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to initialize graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_abr_graph():\n",
    "    g = rdflib.Graph()\n",
    "    g.bind(\"abrc\", \"https://w3id.org/anything-but-routine/4.0/classification/\")\n",
    "    g.bind(\"abri\", \"https://w3id.org/anything-but-routine/4.0/instance/\")\n",
    "    g.bind(\"abrw\", \"https://w3id.org/anything-but-routine/4.0/work/\")\n",
    "    g.bind(\"bf\", \"http://id.loc.gov/ontologies/bibframe/\")\n",
    "    g.bind(\"arm\", \"https://w3id.org/arm/core/ontology/0.1/\")\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.6 s, sys: 2.06 s, total: 25.6 s\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "instances = df.where((pd.notnull(df)), None).to_dict('records')\n",
    "work_to_instance_map = collections.defaultdict(list)\n",
    "\n",
    "for i in instances:\n",
    "    g = initialize_abr_graph()\n",
    "    name = i['abrid']\n",
    "    work_to_instance_map[i['abrid']].append(name)\n",
    "    id = abri[name]\n",
    "    g.add((id, RDF.type, bf.Instance))\n",
    "    g.add((id, RDFS.label, Literal(i['title'])))\n",
    "    g.add((id, bf.instanceOf, abrw[i['work']]))\n",
    "    # bf:classification\n",
    "    g.add((id, bf.classification, abrc['C']))\n",
    "    # bf.contributor\n",
    "    wsb = BNode()\n",
    "    g.add((id, bf.contributor, wsb))\n",
    "    g.add((wsb, RDF.type, bf.Agent))\n",
    "    g.add((wsb, RDF.type, bf.Person))\n",
    "    g.add((wsb, bf.role, Literal(\"author\")))\n",
    "    g.add((wsb, RDFS.label, Literal(\"William S. Burroughs\")))\n",
    "    # bf:title\n",
    "    title = BNode()\n",
    "    g.add((id, bf.title, title))\n",
    "    g.add((title, RDF.type, bf.Title))\n",
    "    g.add((title, RDFS.label, Literal(i['title'])))\n",
    "    # bf:identifiedBy\n",
    "    schottlaender_id = BNode()\n",
    "    g.add((id, bf.identifiedBy, schottlaender_id))\n",
    "    g.add((schottlaender_id, RDF.type, bf.Identifier))\n",
    "    g.add((schottlaender_id, bf.source, Literal(\"Schottlaender v4.0\")))\n",
    "    g.add((schottlaender_id, RDF.value, Literal(i['abrid'])))\n",
    "    m_n_m = i['mnmid']\n",
    "    if m_n_m:\n",
    "        m_n_m_id = BNode()\n",
    "        g.add((id, bf.identifiedBy, m_n_m_id))\n",
    "        g.add((m_n_m_id, RDF.type, bf.Identifier))\n",
    "        g.add((m_n_m_id, bf.source, Literal(\"Maynard & Miles\")))\n",
    "        g.add((m_n_m_id, RDF.value, Literal(m_n_m)))\n",
    "    # bf:note\n",
    "    note = i['note']\n",
    "    if note:\n",
    "        sentences = nltk.sent_tokenize(note)\n",
    "        for sentence in sentences:\n",
    "            n = BNode()\n",
    "            g.add((id, bf.note, n))\n",
    "            g.add((n, RDF.type, bf.Note))\n",
    "            g.add((n, RDF.value, Literal(sentence)))\n",
    "            # Contributors extracted from note\n",
    "            tokenized = nltk.word_tokenize(sentence)\n",
    "            tagged = nltk.pos_tag(tokenized)\n",
    "            namedEnt = nltk.ne_chunk(tagged)\n",
    "            for t in namedEnt.subtrees():\n",
    "                if t.label() == 'PERSON':\n",
    "                    person_tokens = [ c[0] for c in t ]\n",
    "                    person_name = ' '.join(person_tokens)\n",
    "                    person = BNode()\n",
    "                    g.add((id, bf.contributor, person))\n",
    "                    g.add((person, RDF.type, bf.Agent))\n",
    "                    g.add((person, RDF.type, bf.Person)) \n",
    "                    g.add((person, bf.role, Literal(\"contributor\")))\n",
    "                    g.add((person, RDFS.label, Literal(person_name)))\n",
    "    # bf:provisionActivity\n",
    "    date = i['date']\n",
    "    if date:\n",
    "        pa_lit = BNode()\n",
    "        g.add((id, bf.provisionActivity, pa_lit))\n",
    "        g.add((pa_lit, RDF.type, bf.ProvisionActivity))\n",
    "        g.add((pa_lit, RDF.type, bf.Publication))\n",
    "        g.add((pa_lit, bf.date, Literal(date)))\n",
    "    g.serialize(f\"ttl/instance/{name}.ttl\", format='turtle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "works = pd.DataFrame(df[['title', 'abrid']]).drop_duplicates().to_dict('records')\n",
    "\n",
    "for work in works:\n",
    "    g = initialize_abr_graph()\n",
    "    name = work['abrid']\n",
    "    id = abrw[name]\n",
    "    g.add((id, RDF.type, bf.Work))\n",
    "    g.add((id, RDF.type, bf.Text))\n",
    "    g.add((id, RDFS.label, Literal(work['title'])))\n",
    "    g.add((id, bf.classification, abrc['C']))\n",
    "    wsb = BNode()\n",
    "    g.add((id, bf.contributor, wsb))\n",
    "    g.add((wsb, RDF.type, bf.Agent))\n",
    "    g.add((wsb, RDF.type, bf.Person)) \n",
    "    g.add((wsb, bf.role, Literal(\"author\")))\n",
    "    g.add((wsb, RDFS.label, Literal(\"William S. Burroughs\")))\n",
    "    for inst in work_to_instance_map[name]:\n",
    "        g.add((id, bf.hasInstance, abri[inst]))\n",
    "    g.serialize(f\"ttl/work/{name}.ttl\", format='turtle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:wsbhack]",
   "language": "python",
   "name": "conda-env-wsbhack-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
